# Introduction

- Problem with large MDPs:

    - There are too many states and/or actions to store in memory
    
    - It is too slow to learn the value of each state individually

- Solution for large MDPs:

    - Estimate value function with function approximation

    $$\hat{v}(s; w) \approx  v_\pi(s)$$ , or  $$\hat{q}(s, a; w) \approx q_\pi(s, a)$$

    - Generalise from seen states to unseen states

    - Update parameter $$w$$ using MC or TD learning

# Incremental Methods

## Linear Value Function Approximation

Represent state by a feature vector: $$x(S) = (x_1(S), ..., x_n(S))^T$$.

Represent value function by a linear combination of features: $$ \hat{v}(S; w) = x(S)^T w = \sum_{j=1}^{n} x_j(S) w_j $$.

Objective function is quadratic in parameters $$w$$: $$ J(w) = E_{\pi} [(v_{\pi}(S) - x(S)^T w)^2]$$

Stochastic gradient descent converges on global optimum: 

$$ \triangledown_w \hat{v}(S; w) = x(S) $$

$$ \triangle w = \alpha (v_{\pi}(S) - \hat{v}(S; w)) x(S)$$

**Tips:**
ç‰¹åˆ«åœ°, åœ¨æœ‰é™çŠ¶æ€çš„æƒ…å†µä¸‹, å–ç‰¹å¾ä¸º $$ x(S) = (1(S=s_1), 1(S=s_2), ..., 1(S=s_n))^T$$, æ­¤æ—¶çš„linear value function approximation å³ä¸º Table lookup.

## Incremental Prediction Algorithms

å‰é¢çš„åšæ³•ä¸­å‡è®¾äº†å·²ç»çŸ¥é“äº† true value function $$v_\pi(s)$$, ä½†è¿™ä¸ª true value function åœ¨å®é™…çš„é—®é¢˜ä¸­æ˜¯æœªçŸ¥çš„.

In practice, we substitute a target for $$v_\pi(s)$$

### For MC, the target is the return $$G_t$$

$$ \triangle w = \alpha (G_t âˆ’ \hat{v}(S_t; w)) \triangledown_w \hat{v}(S_t; w)$$

Return Gt is an unbiased, noisy sample of true value $$v_\pi(S_t)$$
    
Monte-Carlo evaluation converges to a local optimum, even when using non-linear value function approximation.

### For TD(0), the target is the TD target $$R_{t+1 }+ \gamma \hat{v}(S_{t+1}; w)$$

$$ \triangle w = \alpha (R_{t+1 }+ \gamma \hat{v}(S_{t+1}; w) âˆ’ \hat{v}(S_t; w)) \triangledown_w \hat{v}(S_t; w) $$

The TD(0)-target is a biased sample of true value $$v_\pi(S_t)$$

Linear TD(0) converges (close) to global optimum.
    
### For TD(Î»), the target is the Î»-return $$G_t^{\lambda}$$

The Î»-return is also a biased sample of true value $$v_\pi(S_t)$$.

- Forward view linear TD(Î»):
        
    $$ \triangle w = \alpha (G_t^{\lambda} âˆ’ \hat{v}(S_t; w)) \triangledown_w \hat{v}(S_t; w) $$

- Backward view linear TD(Î»):
    
    $$ \delta_t = R_{t+1} + \gamma \hat{v}(S_{t+1}; w) âˆ’ \hat{v}(St; w) $$
        
    $$ E_t = \gamma \lambda E_{tâˆ’1} + x(S_t) $$
        
    $$ \triangle w = \alpha \delta_t E_t $$


## Incremental Control Algorithms

**Policy evaluation**: Approximate policy evaluation, $$ \hat{q}(Â·, Â·; w) \approx q_\pi $$

ä¸å‰é¢çš„ Incremental Prediction Algorithms ç±»ä¼¼, åªä¸è¿‡è¿™é‡Œæ˜¯å¯¹ $$ \hat{q}(Â·, Â·; w) $$ æ“ä½œ.

**Policy improvement**: $$\epsilon$$-greedy policy improvement

## Convergence

- **Convergence of Prediction Algorithms**

![](/assets/convergence-prediction.png)

- **Convergence of Control Algorithms**

![](/assets/convergence-control.png)


# Batch Methods

Gradient descent is simple and appealing, but it is not sample efficient.

Batch methods seek to find the best fitting value function given the agentâ€™s experience ("training data").

## Least Squares Prediction

**Stochastic Gradient Descent with Experience Replay**

Given experience consisting of < state; value > pairs

$$ D = \{ \left \langle s_1, v_1^\pi \right \langle, ..., \left \langle s_T, v_T^\pi \right \langle \}$$

Repeat:

1. Sample state, value from experience

    $$ \left \langle s, v^\pi \right \langle \sim D $$

2. Apply stochastic gradient descent update

    $$ \triangle w = \alpha (v^\pi âˆ’ \hat{v}(S_t; w)) \triangledown_w \hat{v}(S_t; w)$$

Converges to least squares solution: $$ w^\pi = \arg \min LS(w)$$

## Least Squares Control



















