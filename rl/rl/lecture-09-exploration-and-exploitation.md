# Lecture 09 Exploration and Exploitation

## Introduction

* **Naive Exploration**

  Add noise to greedy policy \(e.g. -greedy\)

* **Optimistic Initialisation**

  Assume the best until proven otherwise

* **Optimism in the Face of Uncertainty**

  Prefer actions with uncertain values

* **Probability Matching**

  Select actions according to probability they are best

* **Information State Search**

  Lookahead search incorporating value of information

## Multi-Armed Bandits

## Contextual Bandits

## MDPs

