# Introduction

- **Naive Exploration**

    Add noise to greedy policy (e.g. -greedy)

- **Optimistic Initialisation**

    Assume the best until proven otherwise

- **Optimism in the Face of Uncertainty**

    Prefer actions with uncertain values

- **Probability Matching**

    Select actions according to probability they are best

- **Information State Search**

    Lookahead search incorporating value of information

# Multi-Armed Bandits

# Contextual Bandits

# MDPs